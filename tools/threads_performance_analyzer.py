#!/usr/bin/env python3
"""
Threads Performance Analyzer - ãƒ¬ã‚¹å’å…ˆè¼©21daysã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ†æãƒ„ãƒ¼ãƒ«

Threads API v1.0ã‚’ä½¿ç”¨ã—ã¦æŠ•ç¨¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æã—ã€
é«˜ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦æ¬¡æœŸæŠ•ç¨¿ã«åæ˜ ã™ã‚‹ã€‚

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
    pip install requests pandas openai python-dotenv

ç’°å¢ƒå¤‰æ•°(.env):
    THREADS_ACCESS_TOKEN=å–å¾—ã—ãŸã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
    THREADS_USER_ID=å–å¾—ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ID
    openai_API=OpenAI APIã‚­ãƒ¼ï¼ˆå­¦ç¿’åˆ†æç”¨ï¼‰

Usage:
    # æŠ•ç¨¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    python threads_performance_analyzer.py --analyze

    # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã®å­¦ç¿’
    python threads_performance_analyzer.py --learn

    # 22æ—¥ç›®ä»¥é™ã®æŠ•ç¨¿ã‚’æ”¹å–„ç”Ÿæˆ
    python threads_performance_analyzer.py --generate --day 22

    # Threads APIèªè¨¼ãƒ˜ãƒ«ãƒ—
    python threads_performance_analyzer.py --setup
"""

import argparse
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
from dotenv import load_dotenv

# Configuration paths
BASE_DIR = Path(__file__).parent.parent
TOOLS_DIR = Path(__file__).parent
ENV_FILE = TOOLS_DIR / ".env"
SCHEDULE_FILE = BASE_DIR / "research_ideas" / "relationship" / "600_posts_schedule.csv"
LEARNING_OUTPUT_DIR = BASE_DIR / "learning"
ANALYSIS_OUTPUT_DIR = BASE_DIR / "analyses"

# Threads API endpoints
THREADS_API_BASE = "https://graph.threads.net/v1.0"


class ThreadsAPIClient:
    """Threads API v1.0 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, access_token: str, user_id: str):
        self.access_token = access_token
        self.user_id = user_id
        self.base_url = THREADS_API_BASE

    def _make_request(
        self, endpoint: str, params: Optional[Dict] = None
    ) -> Dict:
        """API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        if params is None:
            params = {}
        params["access_token"] = self.access_token

        url = f"{self.base_url}/{endpoint}"
        response = requests.get(url, params=params, timeout=30)

        if response.status_code != 200:
            raise Exception(
                f"Threads API Error: {response.status_code} - {response.text}"
            )

        return response.json()

    def get_user_threads(
        self,
        limit: int = 25,
        since: Optional[datetime] = None,
        until: Optional[datetime] = None,
    ) -> List[Dict]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ä¸€è¦§ã‚’å–å¾—

        Args:
            limit: å–å¾—ä»¶æ•°ï¼ˆæœ€å¤§100ï¼‰
            since: å–å¾—é–‹å§‹æ—¥æ™‚
            until: å–å¾—çµ‚äº†æ—¥æ™‚

        Returns:
            æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        params = {
            "fields": "id,text,timestamp,media_type,permalink,is_quote_post",
            "limit": min(limit, 100),
        }

        if since:
            params["since"] = int(since.timestamp())
        if until:
            params["until"] = int(until.timestamp())

        result = self._make_request(f"{self.user_id}/threads", params)
        return result.get("data", [])

    def get_thread_insights(self, thread_id: str) -> Dict:
        """
        æŠ•ç¨¿ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆï¼ˆã„ã„ã­ã€é–²è¦§æ•°ç­‰ï¼‰ã‚’å–å¾—

        Args:
            thread_id: æŠ•ç¨¿ID

        Returns:
            ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        params = {
            "metric": "views,likes,replies,reposts,quotes",
        }

        result = self._make_request(f"{thread_id}/insights", params)

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        insights = {}
        for item in result.get("data", []):
            metric_name = item.get("name")
            values = item.get("values", [{}])
            insights[metric_name] = values[0].get("value", 0) if values else 0

        return insights


class ThreadsPerformanceAnalyzer:
    """ThreadsæŠ•ç¨¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        load_dotenv(ENV_FILE)
        self.access_token = os.getenv("THREADS_ACCESS_TOKEN")
        self.user_id = os.getenv("THREADS_USER_ID")
        self.openai_api = os.getenv("openai_API")

        if self.access_token and self.user_id:
            self.client = ThreadsAPIClient(self.access_token, self.user_id)
        else:
            self.client = None

    def fetch_campaign_posts(
        self, start_date: datetime, end_date: Optional[datetime] = None
    ) -> pd.DataFrame:
        """
        ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æœŸé–“ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            start_date: ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³é–‹å§‹æ—¥
            end_date: çµ‚äº†æ—¥ï¼ˆNoneã®å ´åˆã¯ç¾åœ¨ã¾ã§ï¼‰

        Returns:
            æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®DataFrame
        """
        if not self.client:
            raise ValueError(
                "Threads APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚--setup ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )

        if end_date is None:
            end_date = datetime.now()

        print(f"ğŸ“¥ æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... ({start_date.date()} ~ {end_date.date()})")

        # æŠ•ç¨¿ä¸€è¦§å–å¾—
        threads = self.client.get_user_threads(
            limit=100, since=start_date, until=end_date
        )

        if not threads:
            print("âš ï¸ å¯¾è±¡æœŸé–“ã®æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’å–å¾—ã—ã¦çµåˆ
        records = []
        for i, thread in enumerate(threads):
            print(f"  ã‚¤ãƒ³ã‚µã‚¤ãƒˆå–å¾—ä¸­... {i+1}/{len(threads)}", end="\r")

            try:
                insights = self.client.get_thread_insights(thread["id"])
            except Exception as e:
                print(f"\n  âš ï¸ ã‚¤ãƒ³ã‚µã‚¤ãƒˆå–å¾—å¤±æ•—: {thread['id']} - {e}")
                insights = {}

            record = {
                "thread_id": thread["id"],
                "text": thread.get("text", ""),
                "timestamp": thread.get("timestamp"),
                "media_type": thread.get("media_type", "TEXT"),
                "permalink": thread.get("permalink", ""),
                "views": insights.get("views", 0),
                "likes": insights.get("likes", 0),
                "replies": insights.get("replies", 0),
                "reposts": insights.get("reposts", 0),
                "quotes": insights.get("quotes", 0),
            }
            records.append(record)

        print()  # æ”¹è¡Œ

        df = pd.DataFrame(records)

        # è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df["engagement_total"] = (
            df["likes"] + df["replies"] + df["reposts"] + df["quotes"]
        )
        df["engagement_rate"] = (df["engagement_total"] / (df["views"] + 1)) * 100
        df["posting_hour"] = df["timestamp"].dt.hour
        df["posting_day"] = df["timestamp"].dt.day_name()

        return df

    def analyze_top_performers(
        self, df: pd.DataFrame, top_n: int = 10
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‚’åˆ†æ

        Args:
            df: æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿
            top_n: ä¸Šä½ä½•ä»¶ã‚’æŠ½å‡ºã™ã‚‹ã‹

        Returns:
            (ãƒˆãƒƒãƒ—æŠ•ç¨¿DataFrame, åˆ†æã‚µãƒãƒªãƒ¼Dict)
        """
        if df.empty:
            return pd.DataFrame(), {}

        # 100é–²è¦§ä»¥ä¸Š OR ã„ã„ã­1ä»¥ä¸Šã®ã‚‚ã®ã‚’æŠ½å‡º
        high_perform = df[(df["views"] >= 100) | (df["likes"] >= 1)].copy()

        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã§ã‚½ãƒ¼ãƒˆ
        high_perform = high_perform.sort_values("engagement_rate", ascending=False)

        # åˆ†æã‚µãƒãƒªãƒ¼
        summary = {
            "total_posts": len(df),
            "high_perform_posts": len(high_perform),
            "posts_over_100_views": len(df[df["views"] >= 100]),
            "posts_with_likes": len(df[df["likes"] >= 1]),
            "avg_views": df["views"].mean(),
            "avg_likes": df["likes"].mean(),
            "avg_engagement_rate": df["engagement_rate"].mean(),
            "best_hour": (
                df.groupby("posting_hour")["engagement_rate"].mean().idxmax()
                if not df.empty
                else None
            ),
            "best_day": (
                df.groupby("posting_day")["engagement_rate"].mean().idxmax()
                if not df.empty
                else None
            ),
        }

        return high_perform.head(top_n), summary

    def extract_patterns(self, top_posts: pd.DataFrame) -> Dict:
        """
        é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º

        Args:
            top_posts: ãƒˆãƒƒãƒ—æŠ•ç¨¿ã®DataFrame

        Returns:
            ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ
        """
        if top_posts.empty:
            return {}

        patterns = {
            "common_lengths": [],
            "common_keywords": [],
            "common_structures": [],
            "time_patterns": [],
            "sample_posts": [],
        }

        for _, row in top_posts.iterrows():
            text = row["text"]

            # æ–‡å­—æ•°
            patterns["common_lengths"].append(len(text))

            # æ™‚é–“å¸¯
            patterns["time_patterns"].append(row["posting_hour"])

            # ã‚µãƒ³ãƒ—ãƒ«ï¼ˆä¸Šä½5ä»¶ï¼‰
            if len(patterns["sample_posts"]) < 5:
                patterns["sample_posts"].append(
                    {
                        "text": text[:200] + "..." if len(text) > 200 else text,
                        "views": row["views"],
                        "likes": row["likes"],
                        "engagement_rate": round(row["engagement_rate"], 2),
                    }
                )

        # çµ±è¨ˆ
        patterns["avg_length"] = (
            sum(patterns["common_lengths"]) / len(patterns["common_lengths"])
            if patterns["common_lengths"]
            else 0
        )
        patterns["best_hours"] = list(
            pd.Series(patterns["time_patterns"]).value_counts().head(3).index
        )

        return patterns

    def generate_learning_prompt(
        self, patterns: Dict, summary: Dict, top_posts: pd.DataFrame
    ) -> str:
        """
        å­¦ç¿’çµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›

        Args:
            patterns: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœ
            summary: åˆ†æã‚µãƒãƒªãƒ¼
            top_posts: ãƒˆãƒƒãƒ—æŠ•ç¨¿

        Returns:
            å­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        sample_texts = "\n\n".join(
            [
                f"ã€{i+1}. é–²è¦§{p['views']}å› / ã„ã„ã­{p['likes']}ã€‘\n{p['text']}"
                for i, p in enumerate(patterns.get("sample_posts", []))
            ]
        )

        prompt = f"""# ãƒ¬ã‚¹å’å…ˆè¼© 21daysæŠ•ç¨¿ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿

## ğŸ“Š å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ç·æŠ•ç¨¿æ•°: {summary.get('total_posts', 0)}ä»¶
- é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿: {summary.get('high_perform_posts', 0)}ä»¶
- 100é–²è¦§ä»¥ä¸Š: {summary.get('posts_over_100_views', 0)}ä»¶
- ã„ã„ã­ç²å¾—: {summary.get('posts_with_likes', 0)}ä»¶
- å¹³å‡é–²è¦§æ•°: {summary.get('avg_views', 0):.1f}
- å¹³å‡ã„ã„ã­æ•°: {summary.get('avg_likes', 0):.1f}
- å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {summary.get('avg_engagement_rate', 0):.2f}%

## ğŸ¯ ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³
- æœ€é©ãªæŠ•ç¨¿æ™‚é–“å¸¯: {patterns.get('best_hours', [])}æ™‚å°
- ç†æƒ³çš„ãªæ–‡å­—æ•°: ç´„{patterns.get('avg_length', 0):.0f}æ–‡å­—
- æœ€ã‚‚åå¿œãŒè‰¯ã„æ›œæ—¥: {summary.get('best_day', 'N/A')}

## âœ¨ é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‚µãƒ³ãƒ—ãƒ«
{sample_texts}

## ğŸ’¡ 22æ—¥ç›®ä»¥é™ã®æŠ•ç¨¿æ”¹å–„æŒ‡é‡
ä¸Šè¨˜ã®é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‚’åˆ†æã—ãŸçµæœã€ä»¥ä¸‹ã®è¦ç´ ãŒåŠ¹æœçš„ï¼š

1. **å…·ä½“çš„ãªæ•°å­—ã‚’å«ã‚ã‚‹** - ã€Œå¹´é–“130å›ã€ã®ã‚ˆã†ãªå…·ä½“æ€§
2. **å…±æ„Ÿã‚’èª˜ã†å•ã„ã‹ã‘** - ã€Œã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿã€ã€Œã©ã†ã§ã™ã‹ï¼Ÿã€
3. **é©åº¦ãªæ”¹è¡Œ** - èª­ã¿ã‚„ã™ã•ã‚’é‡è¦–
4. **æ™‚é–“å¸¯ã®æœ€é©åŒ–** - {patterns.get('best_hours', [])}æ™‚å°ã‚’å„ªå…ˆ

---
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        return prompt

    def save_learning_data(self, prompt: str, patterns: Dict, summary: Dict):
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜

        Args:
            prompt: å­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            patterns: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            summary: ã‚µãƒãƒªãƒ¼
        """
        LEARNING_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’Markdownã§ä¿å­˜
        prompt_file = LEARNING_OUTPUT_DIR / f"threads_learning_{timestamp}.md"
        with open(prompt_file, "w", encoding="utf-8") as f:
            f.write(prompt)
        print(f"âœ… å­¦ç¿’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿å­˜: {prompt_file}")

        # JSONå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆ©ç”¨ç”¨ï¼‰
        data_file = LEARNING_OUTPUT_DIR / f"threads_learning_{timestamp}.json"
        with open(data_file, "w", encoding="utf-8") as f:
            json.dump(
                {"summary": summary, "patterns": patterns},
                f,
                ensure_ascii=False,
                indent=2,
            )
        print(f"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {data_file}")

        return prompt_file, data_file


def print_setup_guide():
    """Threads API ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã‚’è¡¨ç¤º"""
    guide = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   Threads API ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€Step 1: Meta for Developersã§ã‚¢ãƒ—ãƒªä½œæˆã€‘
1. https://developers.facebook.com/ ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ã€Œãƒã‚¤ã‚¢ãƒ—ãƒªã€â†’ã€Œã‚¢ãƒ—ãƒªã‚’ä½œæˆã€
3. ã€Œãã®ä»–ã€â†’ã€Œæ¬¡ã¸ã€â†’ã€Œãƒ“ã‚¸ãƒã‚¹ã€é¸æŠ
4. ã‚¢ãƒ—ãƒªåã‚’å…¥åŠ›ã—ã¦ä½œæˆ

ã€Step 2: Threads APIã‚’è¿½åŠ ã€‘
1. ä½œæˆã—ãŸã‚¢ãƒ—ãƒªã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã€Œãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’è¿½åŠ ã€
2. ã€ŒThreads APIã€ã‚’æ¢ã—ã¦ã€Œè¨­å®šã€ã‚’ã‚¯ãƒªãƒƒã‚¯
3. ã€ŒThreads APIã‚’è¿½åŠ ã€

ã€Step 3: ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ã€‘
1. Threads API > Settings ã«ç§»å‹•
2. ã€ŒThreads User Token Generatorã€ã§ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
3. å¿…è¦ãªã‚¹ã‚³ãƒ¼ãƒ—ã‚’é¸æŠ:
   - threads_basic
   - threads_content_publish (æŠ•ç¨¿ç”¨)
   - threads_manage_insights (åˆ†æç”¨)
4. ã€ŒGenerate Tokenã€ã‚’ã‚¯ãƒªãƒƒã‚¯

ã€Step 4: ç’°å¢ƒå¤‰æ•°è¨­å®šã€‘
ä»¥ä¸‹ã‚’ note-articles/tools/.env ã«è¿½åŠ :

THREADS_ACCESS_TOKEN=å–å¾—ã—ãŸã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
THREADS_USER_ID=ã‚ãªãŸã®Threadsãƒ¦ãƒ¼ã‚¶ãƒ¼ID

â€»ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã¯APIçµŒç”±ã§å–å¾—å¯èƒ½:
curl "https://graph.threads.net/v1.0/me?access_token=YOUR_TOKEN"

ã€æ³¨æ„äº‹é …ã€‘
- ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¯60æ—¥ã§æœŸé™åˆ‡ã‚Œï¼ˆé•·æœŸãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›æ¨å¥¨ï¼‰
- æœ¬ç•ªç’°å¢ƒã§ã¯ã‚¢ãƒ—ãƒªå¯©æŸ»ãŒå¿…è¦ãªå ´åˆã‚ã‚Š
- è©³ç´°: https://developers.facebook.com/docs/threads

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
"""
    print(guide)


def simulate_analysis():
    """APIæœªè¨­å®šæ™‚ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰"""
    print("\nğŸ“Š ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã‚’å®Ÿè¡Œ...")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ•ç¨¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    if not SCHEDULE_FILE.exists():
        print(f"âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {SCHEDULE_FILE}")
        return

    df = pd.read_csv(SCHEDULE_FILE)

    # Day 1-15ã®æŠ•ç¨¿ã‚’æŠ½å‡ºï¼ˆ2025-11-22é–‹å§‹ï¼‰
    start_date = datetime(2025, 11, 22)
    today = datetime.now()
    days_passed = (today - start_date).days + 1

    posted = df[df["Day"] <= days_passed]

    print(f"\nã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³çŠ¶æ³ã€‘")
    print(f"  é–‹å§‹æ—¥: {start_date.strftime('%Y-%m-%d')}")
    print(f"  çµŒéæ—¥æ•°: {days_passed}æ—¥ç›®")
    print(f"  æŠ•ç¨¿æ¸ˆã¿äºˆå®š: {len(posted)}ä»¶")
    print(f"  æ®‹ã‚ŠæŠ•ç¨¿: {len(df) - len(posted)}ä»¶")

    # æŠ•ç¨¿ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†å¸ƒ
    print(f"\nã€æŠ•ç¨¿ã‚¿ã‚¤ãƒ—åˆ†å¸ƒã€‘")
    type_counts = posted["Type"].value_counts()
    for t, count in type_counts.items():
        print(f"  {t}: {count}ä»¶")

    # æ™‚é–“å¸¯åˆ†å¸ƒ
    print(f"\nã€æŠ•ç¨¿æ™‚é–“å¸¯åˆ†å¸ƒã€‘")
    time_counts = posted["Time"].value_counts().sort_index()
    for t, count in time_counts.items():
        print(f"  {t}: {count}ä»¶")

    print(
        """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸  Threads APIæœªè¨­å®šã®ãŸã‚ã€å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã§ãã¾ã›ã‚“        â•‘
â•‘                                                                              â•‘
â•‘  å®Ÿéš›ã®ã€Œã„ã„ã­ã€ã€Œé–²è¦§æ•°ã€ã‚’åˆ†æã™ã‚‹ã«ã¯:                                      â•‘
â•‘  python threads_performance_analyzer.py --setup                              â•‘
â•‘                                                                              â•‘
â•‘  ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    )


def main():
    parser = argparse.ArgumentParser(
        description="Threads Performance Analyzer - ãƒ¬ã‚¹å’å…ˆè¼©21daysã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ†æ"
    )
    parser.add_argument("--setup", action="store_true", help="Threads APIã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰è¡¨ç¤º")
    parser.add_argument("--analyze", action="store_true", help="æŠ•ç¨¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")
    parser.add_argument("--learn", action="store_true", help="é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã‹ã‚‰å­¦ç¿’")
    parser.add_argument("--simulate", action="store_true", help="APIãªã—ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ")
    parser.add_argument("--day", type=int, help="æ”¹å–„æŠ•ç¨¿ç”Ÿæˆå¯¾è±¡ã®æ—¥æ•°")

    args = parser.parse_args()

    if args.setup:
        print_setup_guide()
        return

    if args.simulate or (not args.analyze and not args.learn):
        simulate_analysis()
        return

    # å®Ÿéš›ã®APIåˆ†æ
    analyzer = ThreadsPerformanceAnalyzer()

    if not analyzer.client:
        print("âŒ Threads APIèªè¨¼æƒ…å ±ãŒæœªè¨­å®šã§ã™")
        print("   --setup ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print("   ã¾ãŸã¯ --simulate ã§ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
        return

    if args.analyze or args.learn:
        # ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³é–‹å§‹æ—¥ï¼ˆ2025-11-22ï¼‰
        start_date = datetime(2025, 11, 22)

        try:
            df = analyzer.fetch_campaign_posts(start_date)

            if df.empty:
                print("âŒ åˆ†æå¯¾è±¡ã®æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return

            top_posts, summary = analyzer.analyze_top_performers(df)
            patterns = analyzer.extract_patterns(top_posts)

            print("\n" + "=" * 60)
            print("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
            print("=" * 60)
            for key, value in summary.items():
                print(f"  {key}: {value}")

            if args.learn:
                prompt = analyzer.generate_learning_prompt(patterns, summary, top_posts)
                prompt_file, data_file = analyzer.save_learning_data(
                    prompt, patterns, summary
                )

                print("\n" + "=" * 60)
                print("ğŸ“ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                print("=" * 60)
                print(f"\n22æ—¥ç›®ä»¥é™ã®æŠ•ç¨¿æ”¹å–„ã«ä½¿ç”¨ã—ã¦ãã ã•ã„:")
                print(f"  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt_file}")
                print(f"  ãƒ‡ãƒ¼ã‚¿: {data_file}")

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    main()
