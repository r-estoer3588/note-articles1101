#!/usr/bin/env python3
"""
è¨˜äº‹å“è³ªè‡ªå‹•è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- ã‚¿ã‚¤ãƒˆãƒ«ã®é©åˆ‡æ€§ãƒã‚§ãƒƒã‚¯
- æ–‡å­—æ•°ãƒ»æ§‹æˆãƒã‚§ãƒƒã‚¯
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š
- æ”¹å–„å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ç®—å‡º
"""

import json
import re
from pathlib import Path
from collections import defaultdict


# NG ãƒ¯ãƒ¼ãƒ‰ï¼ˆåº•è¾ºå±¤å‘ã‘ï¼‰
BOTTOM_TIER_WORDS = [
    'ãƒ‘ãƒ', 'ç«¶é¦¬', 'ç«¶è‰‡', 'ç«¶è¼ª', 'ã‚®ãƒ£ãƒ³ãƒ–ãƒ«', 'ã‚¿ãƒã‚³', 'é…’',
    'ã‚³ãƒ³ãƒ“ãƒ‹', 'åº•è¾º', 'ãƒã‚«', 'ã‚«ãƒ¢', 'ã’ã™', 'ãƒã‚¸ã§', 'ãƒ›ã‚²ãƒ¼',
    'ãŠå‰', 'ä¿º', 'ã‚¬ãƒã§', 'ã¶ã£ã¡ã‚ƒã‘'
]

# OK ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¼šç¤¾å“¡å‘ã‘ï¼‰
TARGET_WORDS = [
    'ä¼šç¤¾å“¡', '30ä»£', 'å®ŸéŒ²', 'æã—ãŸ', 'ç¨é‡‘', 'ç¢ºå®šç”³å‘Š',
    'å‰¯æ¥­', 'ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹', 'æŠ•è³‡', 'NISA', 'iDeCo',
    'è»¢è·', 'ã‚­ãƒ£ãƒªã‚¢', 'å¹´å', 'ç¯€ç¨', 'æ³•äººè¨­ç«‹'
]

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
EVIDENCE_WORDS = [
    'å›½ç¨åº', 'é‡‘èåº', 'åšç”ŸåŠ´åƒçœ', 'ç·å‹™çœ', 'çµ±è¨ˆ',
    'èª¿æŸ»', 'ãƒ‡ãƒ¼ã‚¿', 'æ³•å¾‹', 'æ¡æ–‡', 'åˆ¶åº¦'
]


def analyze_article(file_path):
    """å€‹åˆ¥è¨˜äº‹ã‚’è§£æ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return None
    
    # åŸºæœ¬æƒ…å ±æŠ½å‡º
    lines = content.split('\n')
    title = ''
    for line in lines:
        if line.startswith('# '):
            title = line.replace('# ', '').strip()
            break
    
    # ç„¡æ–™éƒ¨åˆ†ã¨æœ‰æ–™éƒ¨åˆ†ã‚’åˆ†é›¢
    free_part = ''
    paid_part = ''
    
    if 'ã€æœ‰æ–™éƒ¨åˆ†ã€‘' in content or '## ã€æœ‰æ–™éƒ¨åˆ†ã€‘' in content:
        parts = re.split(r'##?\s*ã€æœ‰æ–™éƒ¨åˆ†ã€‘', content)
        free_part = parts[0]
        paid_part = parts[1] if len(parts) > 1 else ''
    else:
        free_part = content
    
    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    score = {
        'title_quality': 0,  # ã‚¿ã‚¤ãƒˆãƒ«å“è³ªï¼ˆ0-10ï¼‰
        'target_match': 0,   # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé©åˆåº¦ï¼ˆ0-10ï¼‰
        'evidence_level': 0,  # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ï¼ˆ0-10ï¼‰
        'structure_score': 0,  # æ§‹é€ ã‚¹ã‚³ã‚¢ï¼ˆ0-10ï¼‰
        'total': 0
    }
    
    # 1. ã‚¿ã‚¤ãƒˆãƒ«å“è³ª
    if 'ã€å®ŸéŒ²ã€‘' in title or 'ã€' in title:
        score['title_quality'] += 3
    if any(word in title for word in ['æã—ãŸ', 'å¤±æ•—', 'ç½ ']):
        score['title_quality'] += 3
    if re.search(r'\d+ä¸‡å††', title):
        score['title_quality'] += 2
    if 'ï½œ' in title:  # ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹
        score['title_quality'] += 2
    
    # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé©åˆåº¦
    bottom_count = sum(1 for word in BOTTOM_TIER_WORDS if word in content)
    target_count = sum(1 for word in TARGET_WORDS if word in content)
    
    if target_count > bottom_count:
        score['target_match'] = min(10, target_count * 2)
    else:
        score['target_match'] = max(0, 10 - bottom_count)
    
    # 3. ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«
    evidence_count = sum(1 for word in EVIDENCE_WORDS if word in content)
    score['evidence_level'] = min(10, evidence_count * 3)
    
    # 4. æ§‹é€ ã‚¹ã‚³ã‚¢
    free_length = len(free_part)
    if 800 <= free_length <= 1500:
        score['structure_score'] += 4
    elif free_length > 500:
        score['structure_score'] += 2
    
    if paid_part and len(paid_part) > 1000:
        score['structure_score'] += 3
    
    if '### ' in content:  # è¦‹å‡ºã—æ§‹é€ ãŒã‚ã‚‹
        score['structure_score'] += 3
    
    # ç·åˆã‚¹ã‚³ã‚¢
    score['total'] = sum([
        score['title_quality'],
        score['target_match'],
        score['evidence_level'],
        score['structure_score']
    ]) / 4
    
    return {
        'title': title,
        'file': str(file_path),
        'free_length': free_length,
        'paid_length': len(paid_part),
        'total_length': len(content),
        'bottom_words': bottom_count,
        'target_words': target_count,
        'evidence_words': evidence_count,
        'score': score,
        'issues': []
    }


def categorize_article(analysis):
    """è¨˜äº‹ã‚’åˆ†é¡"""
    score = analysis['score']['total']
    issues = []
    
    # å•é¡Œç‚¹ã®åˆ—æŒ™
    if analysis['bottom_words'] > 5:
        issues.append(f"åº•è¾ºèªå½™å¤šæ•°ï¼ˆ{analysis['bottom_words']}å€‹ï¼‰")
    
    if analysis['target_words'] < 3:
        issues.append("ä¼šç¤¾å“¡å‘ã‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸è¶³")
    
    if analysis['evidence_words'] == 0:
        issues.append("ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ä¸è¶³")
    
    if analysis['free_length'] < 500:
        issues.append(f"ç„¡æ–™éƒ¨åˆ†ãŒçŸ­ã„ï¼ˆ{analysis['free_length']}æ–‡å­—ï¼‰")
    
    if analysis['paid_length'] < 1000:
        issues.append(f"æœ‰æ–™éƒ¨åˆ†ãŒçŸ­ã„ï¼ˆ{analysis['paid_length']}æ–‡å­—ï¼‰")
    
    analysis['issues'] = issues
    
    # åˆ†é¡
    if score >= 7:
        return 'OK', analysis
    elif score >= 4:
        return 'è¦æ”¹å–„', analysis
    else:
        return 'å‰Šé™¤å€™è£œ', analysis


def main():
    print("=" * 80)
    print("ğŸ“Š ã’ã™ã„ã¬è¨˜äº‹å“è³ªè‡ªå‹•è¨ºæ–­")
    print("=" * 80)
    
    base_dir = Path(__file__).parent.parent
    master_path = base_dir / "article_master.json"
    
    # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(master_path, 'r', encoding='utf-8') as f:
        master = json.load(f)
    
    print(f"\nğŸ“š å¯¾è±¡è¨˜äº‹: {len(master['articles'])}æœ¬")
    print("\nğŸ” è§£æä¸­...")
    
    results = {
        'OK': [],
        'è¦æ”¹å–„': [],
        'å‰Šé™¤å€™è£œ': []
    }
    
    for article in master['articles']:
        article_id = article['id']
        file_path = base_dir / (article['file'] + '.md')
        
        if not file_path.exists():
            continue
        
        analysis = analyze_article(file_path)
        if analysis:
            analysis['id'] = article_id
            analysis['master_title'] = article['title']
            category, analysis = categorize_article(analysis)
            results[category].append(analysis)
    
    # çµæœè¡¨ç¤º
    print(f"\n" + "=" * 80)
    print("ğŸ“‹ è¨ºæ–­çµæœ:")
    print(f"  âœ… OK: {len(results['OK'])}æœ¬")
    print(f"  âš ï¸  è¦æ”¹å–„: {len(results['è¦æ”¹å–„'])}æœ¬")
    print(f"  âŒ å‰Šé™¤å€™è£œ: {len(results['å‰Šé™¤å€™è£œ'])}æœ¬")
    print("=" * 80)
    
    # å‰Šé™¤å€™è£œã‚’è¡¨ç¤º
    if results['å‰Šé™¤å€™è£œ']:
        print(f"\nâŒ å‰Šé™¤å€™è£œï¼ˆ{len(results['å‰Šé™¤å€™è£œ'])}æœ¬ï¼‰:")
        for item in sorted(results['å‰Šé™¤å€™è£œ'],
                          key=lambda x: x['score']['total'])[:20]:
            print(f"  ID{item['id']:03d}: {item['master_title'][:40]}")
            print(f"    ã‚¹ã‚³ã‚¢: {item['score']['total']:.1f}/10")
            print(f"    å•é¡Œ: {', '.join(item['issues'][:3])}")
    
    # è¦æ”¹å–„ã‚’å„ªå…ˆåº¦é †ã«è¡¨ç¤º
    if results['è¦æ”¹å–„']:
        print(f"\nâš ï¸  è¦æ”¹å–„ï¼ˆå„ªå…ˆåº¦é †TOP20ï¼‰:")
        sorted_items = sorted(results['è¦æ”¹å–„'],
                             key=lambda x: x['score']['total'])
        for item in sorted_items[:20]:
            print(f"  ID{item['id']:03d}: {item['master_title'][:40]}")
            print(f"    ã‚¹ã‚³ã‚¢: {item['score']['total']:.1f}/10")
            print(f"    å•é¡Œ: {', '.join(item['issues'][:2])}")
    
    # OKãƒªã‚¹ãƒˆã‚‚è¡¨ç¤º
    if results['OK']:
        print(f"\nâœ… é«˜å“è³ªè¨˜äº‹ï¼ˆTOP10ï¼‰:")
        sorted_items = sorted(results['OK'],
                             key=lambda x: x['score']['total'],
                             reverse=True)
        for item in sorted_items[:10]:
            print(f"  ID{item['id']:03d}: {item['master_title'][:40]}")
            print(f"    ã‚¹ã‚³ã‚¢: {item['score']['total']:.1f}/10")
    
    # æ”¹å–„æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    print(f"\n" + "=" * 80)
    print("ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
    print(f"  1. å‰Šé™¤å€™è£œ{len(results['å‰Šé™¤å€™è£œ'])}æœ¬ã‚’å‰Šé™¤ or å¤§å¹…æ›¸ãç›´ã—")
    print(f"  2. è¦æ”¹å–„{len(results['è¦æ”¹å–„'])}æœ¬ã‚’å„ªå…ˆåº¦é †ã«ä¿®æ­£")
    print(f"  3. OK{len(results['OK'])}æœ¬ã‚’ãƒ™ãƒ¼ã‚¹ã«å“è³ªåŸºæº–ã‚’ç¢ºç«‹")
    print("=" * 80)
    
    # JSONå‡ºåŠ›
    output_path = base_dir / "tools" / "quality_report.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {output_path}")


if __name__ == "__main__":
    main()
